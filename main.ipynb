{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaa69a9-52e0-4953-b500-306fa70aa381",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb61beea-4dd5-4aaf-a7ee-16b5dcaf406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48922bf8-0a25-4a22-9965-1b5813dedf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03366f14-5481-4807-9cbb-e29c0862f9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab630f3-26e3-401c-8c6e-c4a8355199e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip util "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ab213de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NadiadAdmin\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\NadiadAdmin\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import wandb\n",
    "import numpy as np\n",
    "import librosa\n",
    "from datasets import DatasetDict, load_dataset\n",
    "from transformers import (\n",
    "    HubertForSequenceClassification,\n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    ")\n",
    "# from utils import collator\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s | %(levelname)s: %(message)s\", level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fa796c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['file', 'label'],\n",
      "        num_rows: 7442\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ROOT = \"C:/Users/NadiadAdmin/Desktop/Audio Emotion project/medium article\"\n",
    "dataset_config = {\n",
    "    \"LOADING_SCRIPT_FILES\": os.path.join(PROJECT_ROOT, \"crema.py\"),\n",
    "    \"CONFIG_NAME\": \"clean\",\n",
    "    \"DATA_DIR\": os.path.join(PROJECT_ROOT, \"crema-d.zip\"),\n",
    "    \"CACHE_DIR\": os.path.join(PROJECT_ROOT, \"cache_crema\"),\n",
    "}\n",
    "\n",
    "ds = load_dataset(\n",
    "    dataset_config[\"LOADING_SCRIPT_FILES\"],\n",
    "    dataset_config[\"CONFIG_NAME\"],\n",
    "    data_dir=dataset_config[\"DATA_DIR\"],\n",
    "    cache_dir=dataset_config[\"CACHE_DIR\"],\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ab30d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wav2Vec2FeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0,\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "model = \"facebook/hubert-base-ls960\"\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model)\n",
    "print(feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31ae76c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NadiadAdmin\\anaconda3\\Lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of HubertForSequenceClassification were not initialized from the model checkpoint at facebook/hubert-large-ls960-ft and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of labels: 2\n"
     ]
    }
   ],
   "source": [
    "model_path = \"facebook/hubert-large-ls960-ft\"\n",
    "hubert_model = HubertForSequenceClassification.from_pretrained(model_path)\n",
    "hubert_model_config = hubert_model.config\n",
    "print(\"Num of labels:\", hubert_model_config.num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "115963fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of HubertForSequenceClassification were not initialized from the model checkpoint at facebook/hubert-base-ls960 and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import HubertConfig, HubertForSequenceClassification\n",
    "NUM_LABELS = 6\n",
    "model_id = \"facebook/hubert-base-ls960\"\n",
    "\n",
    "config = HubertConfig.from_pretrained(model_id, num_labels=NUM_LABELS)\n",
    "hubert_model = HubertForSequenceClassification.from_pretrained(\n",
    "    model_id,\n",
    "    config=config,  # because we need to update num_labels as per our dataset\n",
    "    ignore_mismatched_sizes=True,  # to avoid classifier size mismatch from from_pretrained.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c0ea7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all layers to begin with\n",
    "for param in hubert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# freeze two encoder layers    \n",
    "layers_freeze_num = 2\n",
    "n_layers = (\n",
    "    4 + layers_freeze_num * 16\n",
    ")  # 4 refers to projector and classifier's weights and biases.\n",
    "for name, param in list(hubert_model.named_parameters())[-n_layers:]:\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d51581c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def process_audio(example):\n",
    "    import librosa\n",
    "    # Load the audio file with librosa\n",
    "    audio, _ = librosa.load(example[\"file\"], sr=16000, mono=False)\n",
    "    # Return the modified example with the audio array\n",
    "    return {\"array\": audio}\n",
    "\n",
    "# Assume ds is your dataset\n",
    "ds = ds.map(process_audio, num_proc=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf13f8b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['file', 'label', 'array'],\n",
      "        num_rows: 7442\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cb5df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set()\n",
    "for example in ds['train']:\n",
    "    unique_labels.add(example['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf218745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73c4bae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Assuming `ds` is your DatasetDict object as shown in your message\n",
    "# Step 1: Define the label mapping\n",
    "label_to_int = {\n",
    "    \"SAD\": 0,\n",
    "    \"FEA\": 1,\n",
    "    \"HAP\": 2,\n",
    "    \"NEU\": 3,\n",
    "    \"ANG\": 4,\n",
    "    \"DIS\": 5\n",
    "}\n",
    "\n",
    "# Step 2: Convert labels in the dataset\n",
    "def convert_labels(example):\n",
    "    example['label'] = label_to_int[example['label']]\n",
    "    return example\n",
    "\n",
    "ds['train'] = ds['train'].map(convert_labels)\n",
    "\n",
    "# Step 3: Verify the conversion\n",
    "for example in ds['train']:\n",
    "    print(example['label'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a7cfdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS THE DATASET TO THE FORMAT EXPECTED BY THE MODEL FOR TRAINING\n",
    "\n",
    "INPUT_FIELD = \"input_values\"\n",
    "LABEL_FIELD = \"labels\"\n",
    "\n",
    "def prepare_dataset(batch, feature_extractor):\n",
    "    audio_arr = batch[\"array\"]\n",
    "    input = feature_extractor(\n",
    "        audio_arr, sampling_rate=16000, padding=True, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    batch[INPUT_FIELD] = input.input_values[0]\n",
    "    batch[LABEL_FIELD] = int(batch[\"label\"])  # colname MUST be labels as Trainer will look for it by default\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1063ded4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-16 07:45:16,531 | INFO: Finished extracting features from audio arrays.\n"
     ]
    }
   ],
   "source": [
    "# APPLY THE DATA PREP USING FEATURE EXTRACTOR TO ALL EXAMPLES\n",
    "ds = ds.map(\n",
    "    prepare_dataset,\n",
    "    fn_kwargs={\"feature_extractor\": feature_extractor},\n",
    "    # num_proc=2,\n",
    ")\n",
    "logging.info(\"Finished extracting features from audio arrays.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5729996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL TO ID\n",
    "ds = ds.class_encode_column(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cc1e34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTRODUCE TRAIN TEST VAL SPLITS\n",
    "\n",
    "# 90% train, 10% test + validation\n",
    "train_testvalid = ds[\"train\"].train_test_split(shuffle=True, test_size=0.1)\n",
    "# Split the 10% test + valid in half test, half valid\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "ds = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'val': test_valid['train']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5f2e7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NadiadAdmin\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer_config = {\n",
    "  \"OUTPUT_DIR\": \"results\",\n",
    "  \"TRAIN_EPOCHS\": 20,\n",
    "  \"TRAIN_BATCH_SIZE\": 8,\n",
    "  \"EVAL_BATCH_SIZE\": 8,\n",
    "  \"GRADIENT_ACCUMULATION_STEPS\": 4,\n",
    "  \"WARMUP_STEPS\": 500,\n",
    "  \"DECAY\": 0.01,\n",
    "  \"LOGGING_STEPS\": 10,\n",
    "  \"MODEL_DIR\": \"models/test-hubert-model\",\n",
    "  \"SAVE_STEPS\": 100\n",
    "}\n",
    "\n",
    "# Fine-Tuning with Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=trainer_config[\"OUTPUT_DIR\"],  # output directory\n",
    "    gradient_accumulation_steps=trainer_config[\n",
    "        \"GRADIENT_ACCUMULATION_STEPS\"\n",
    "    ],  # accumulate the gradients before running optimization step\n",
    "    num_train_epochs=trainer_config[\n",
    "        \"TRAIN_EPOCHS\"\n",
    "    ],  # total number of training epochs\n",
    "    per_device_train_batch_size=trainer_config[\n",
    "        \"TRAIN_BATCH_SIZE\"\n",
    "    ],  # batch size per device during training\n",
    "    per_device_eval_batch_size=trainer_config[\n",
    "        \"EVAL_BATCH_SIZE\"\n",
    "    ],  # batch size for evaluation\n",
    "    warmup_steps=trainer_config[\n",
    "        \"WARMUP_STEPS\"\n",
    "    ],  # number of warmup steps for learning rate scheduler\n",
    "    save_steps=trainer_config[\"SAVE_STEPS\"], # save checkpoint every 100 steps\n",
    "    weight_decay=trainer_config[\"DECAY\"],  # strength of weight decay\n",
    "    logging_steps=trainer_config[\"LOGGING_STEPS\"],\n",
    "    evaluation_strategy=\"epoch\",  # report metric at end of each epoch\n",
    "    report_to=\"wandb\",  # enable logging to W&B\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d000376",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d25d8067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "import torch\n",
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "INPUT_FIELD = \"input_values\"\n",
    "LABEL_FIELD = \"labels\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(\n",
    "        self, examples: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        input_features = [\n",
    "            {INPUT_FIELD: example[INPUT_FIELD]} for example in examples\n",
    "        ]  # example is basically row0, row1, etc...\n",
    "        labels = [example[LABEL_FIELD] for example in examples]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        batch[LABEL_FIELD] = torch.tensor(labels)\n",
    "\n",
    "        return batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b8ad548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE DATA COLLATOR - TO PAD TRAINING BATCHES DYNAMICALLY\n",
    "data_collator = DataCollatorCTCWithPadding(\n",
    "            processor=feature_extractor,\n",
    "            padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63d420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c607da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_metric\n",
    "\n",
    "from evaluate import load\n",
    "def compute_metrics(eval_pred):\n",
    "    # DEFINE EVALUATION METRIC\n",
    "    compute_accuracy_metric = load(\"accuracy\")\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return compute_accuracy_metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5000a78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TRAINING\n",
    "trainer = Trainer(\n",
    "    model=hubert_model,  # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,  # training arguments, defined above\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=ds[\"train\"],  # training dataset\n",
    "    eval_dataset=ds[\"val\"],  # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2413b7c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaheeshpurohit\u001b[0m (\u001b[33mmaheeshpurohit-vellore-institute-of-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\NadiadAdmin\\Desktop\\Audio Emotion project\\medium article\\wandb\\run-20240816_074532-9ez1s6y0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/maheeshpurohit-vellore-institute-of-technology/huggingface/runs/9ez1s6y0' target=\"_blank\">results</a></strong> to <a href='https://wandb.ai/maheeshpurohit-vellore-institute-of-technology/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/maheeshpurohit-vellore-institute-of-technology/huggingface' target=\"_blank\">https://wandb.ai/maheeshpurohit-vellore-institute-of-technology/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/maheeshpurohit-vellore-institute-of-technology/huggingface/runs/9ez1s6y0' target=\"_blank\">https://wandb.ai/maheeshpurohit-vellore-institute-of-technology/huggingface/runs/9ez1s6y0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4180' max='4180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4180/4180 30:22:31, Epoch 19/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.595800</td>\n",
       "      <td>1.495933</td>\n",
       "      <td>0.413978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.123700</td>\n",
       "      <td>0.945449</td>\n",
       "      <td>0.623656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.957500</td>\n",
       "      <td>0.805183</td>\n",
       "      <td>0.690860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.878300</td>\n",
       "      <td>0.761890</td>\n",
       "      <td>0.693548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.899600</td>\n",
       "      <td>0.763145</td>\n",
       "      <td>0.709677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.749400</td>\n",
       "      <td>0.727967</td>\n",
       "      <td>0.736559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.648100</td>\n",
       "      <td>0.719995</td>\n",
       "      <td>0.739247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.734800</td>\n",
       "      <td>0.730048</td>\n",
       "      <td>0.736559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.677200</td>\n",
       "      <td>0.713877</td>\n",
       "      <td>0.728495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.706700</td>\n",
       "      <td>0.705777</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.633900</td>\n",
       "      <td>0.704394</td>\n",
       "      <td>0.747312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4180, training_loss=0.8973172523188249, metrics={'train_runtime': 109391.3862, 'train_samples_per_second': 1.224, 'train_steps_per_second': 0.038, 'total_flos': 4.0564153651911926e+18, 'train_loss': 0.8973172523188249, 'epoch': 19.952267303102627})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bf04be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions=trainer.predict(ds['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c65292a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-17 14:22:25,913 | INFO: Test Set Result: {'test_loss': 0.6547810435295105, 'test_accuracy': 0.7721179624664879, 'test_runtime': 137.2306, 'test_samples_per_second': 2.718, 'test_steps_per_second': 0.342}\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Test Set Result: {}\".format(predictions.metrics))\n",
    "wandb.log({\"test_accuracy\": predictions.metrics[\"test_accuracy\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a201b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
